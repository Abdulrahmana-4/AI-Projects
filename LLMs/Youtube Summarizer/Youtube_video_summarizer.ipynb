{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEt9grgrB3sl",
        "outputId": "dbcc0ffb-1364-4b90-a6f2-261c88be207b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-1.2.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube_transcript_api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube_transcript_api) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (2025.10.5)\n",
            "Downloading youtube_transcript_api-1.2.2-py3-none-any.whl (485 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/485.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m481.3/485.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.0/485.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-1.2.2\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube_transcript_api\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPUgJOy1WMkh",
        "outputId": "ca8e63a0-8219-40da-c9af-666219a4d1dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.78)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3eUuofiWvJR",
        "outputId": "d7c15324-da53-4d64-d266-a47bcd42f3fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "UZ_tEPGztm2g"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import re\n",
        "import contractions # pip install contractions\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "import gradio as gr\n",
        "from groq import Groq\n",
        "import os, getpass"
      ],
      "metadata": {
        "id": "AClOtvekCEeQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting a YouTube transcript from a video URL"
      ],
      "metadata": {
        "id": "wzzChBCvLZUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_id(url):\n",
        "  match = re.search(r\"(?:v=|\\/)([0-9A-Za-z_-]{11})\", url)\n",
        "  return match.group(1) if match else None"
      ],
      "metadata": {
        "id": "r0tWjsMpCxNp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_id = get_video_id(\"https://youtu.be/N_OOfkEWcOk?si=kNUY-0fRKTttqJ7n\")"
      ],
      "metadata": {
        "id": "8B66d7TECa_G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yt = YouTubeTranscriptApi()\n",
        "transcript = yt.fetch(video_id=video_id)"
      ],
      "metadata": {
        "id": "Kz6n_D_iED5v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bleur5opEF4k",
        "outputId": "8ce5ffc0-8944-4dc2-ee9a-9c6e2e3697d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FetchedTranscript(snippets=[FetchedTranscriptSnippet(text=\"Hi, my name is Rohan and I'm a senior solutions\\xa0\\narchitect working on Enterprise Generative AI at\\xa0\\xa0\", start=4.08, duration=5.44), FetchedTranscriptSnippet(text=\"NVIDIA. Large language models, or LLMs, are taking\\xa0\\nthe world by storm. And now, they're easier to\\xa0\\xa0\", start=9.52, duration=7.16), FetchedTranscriptSnippet(text=\"develop and deploy than ever, even without needing\\xa0\\nto host your own GPU infrastructure. Today, I'll\\xa0\\xa0\", start=16.68, duration=6.08), FetchedTranscriptSnippet(text='be showing you how to design an enterprise-grade\\xa0\\nretrieval augmented generation, or RAG, pipeline\\xa0\\xa0', start=22.76, duration=6.48), FetchedTranscriptSnippet(text=\"using NVIDIA's AI Foundation models. Since all of\\xa0\\nthe embedding and generation happens on NVIDIA AI\\xa0\\xa0\", start=29.24, duration=5.96), FetchedTranscriptSnippet(text=\"Foundation endpoints, no local GPU is required.\\xa0\\nIf you'd like to experiment with open-source LLMs\\xa0\\xa0\", start=35.2, duration=6.12), FetchedTranscriptSnippet(text='accelerated by the latest NVIDIA GPUs, check out\\xa0\\nthese endpoints. We host a collection of models\\xa0\\xa0', start=41.32, duration=6.08), FetchedTranscriptSnippet(text='you can interact with through our UI. NVIDIA NeVA,\\xa0\\nthe NEMO visual and language assistant model,\\xa0\\xa0', start=47.4, duration=7.12), FetchedTranscriptSnippet(text='is a great example. We can use NeVA for multimodal\\xa0\\nimage and language understanding through the UI,\\xa0\\xa0', start=54.52, duration=6.44), FetchedTranscriptSnippet(text=\"but we can also access NVIDIA endpoints in our\\xa0\\napplications through API calls. Let's build a\\xa0\\xa0\", start=60.96, duration=6.08), FetchedTranscriptSnippet(text=\"local RAG application that uses our AI foundation\\xa0\\nmodels. We'll need four main components. Firstly,\\xa0\\xa0\", start=67.04, duration=7.28), FetchedTranscriptSnippet(text='a custom data loader for segmenting documents.\\xa0\\nSecondly, a text embedding model for converting\\xa0\\xa0', start=74.32, duration=6.04), FetchedTranscriptSnippet(text='document chunks into vectors. Third, a\\xa0\\nvector database for storing these embeddings,\\xa0\\xa0', start=80.36, duration=5.76), FetchedTranscriptSnippet(text='and finally, a large language model for\\xa0\\nresponse generation. In this example,\\xa0\\xa0', start=86.12, duration=5.44), FetchedTranscriptSnippet(text=\"we'll be using a LangChain connector to NVIDIA's\\xa0\\nAI Foundation endpoints. So if you're already\\xa0\\xa0\", start=91.56, duration=5.2), FetchedTranscriptSnippet(text=\"working with popular open-source frameworks like\\xa0\\nLangChain and LlamaIndex, you don't have to start\\xa0\\xa0\", start=96.76, duration=5.16), FetchedTranscriptSnippet(text=\"over, since NVIDIA upstreams and maintains the\\xa0\\nconnectors. Let's get started. The first step is\\xa0\\xa0\", start=101.92, duration=6.2), FetchedTranscriptSnippet(text='to make an API key for NGC. Click through to this\\xa0\\npage where you can sign up to use the APIs. After\\xa0\\xa0', start=108.12, duration=9.32), FetchedTranscriptSnippet(text='you\\'ve logged in, click on \"Generate API Key\"\\xa0\\nto create a key you can use in your code. Now,\\xa0\\xa0', start=117.44, duration=6.16), FetchedTranscriptSnippet(text=\"let's build the chat UI. We'll use Streamlit,\\xa0\\na popular open-source framework for building\\xa0\\xa0\", start=123.6, duration=5.64), FetchedTranscriptSnippet(text=\"and sharing machine learning and data science\\xa0\\nweb apps. First, we'll focus on the custom data\\xa0\\xa0\", start=129.24, duration=5.92), FetchedTranscriptSnippet(text='connector. Here, we have Streamlit code in Python\\xa0\\nto create a directory for uploading documents,\\xa0\\xa0', start=135.16, duration=6.08), FetchedTranscriptSnippet(text='a browse and upload form, and a successful upload\\xa0\\nbanner. Next, we look at the text embedding model.\\xa0\\xa0', start=141.24, duration=6.36), FetchedTranscriptSnippet(text=\"The model is hosted on NVIDIA's AI Foundation\\xa0\\nendpoints, so we can access it via the LangChain\\xa0\\xa0\", start=147.6, duration=6.0), FetchedTranscriptSnippet(text=\"connector. Now, let's deploy a vector database\\xa0\\nto index our embeddings. In this code snippet,\\xa0\\xa0\", start=153.6, duration=7.72), FetchedTranscriptSnippet(text=\"we'll create a vector store or load a saved\\xa0\\none from memory. To create a new vector store,\\xa0\\xa0\", start=161.32, duration=5.72), FetchedTranscriptSnippet(text='split the documents into chunks based on a certain\\xa0\\nnumber of characters and then use the FAISS\\xa0\\xa0', start=167.04, duration=5.12), FetchedTranscriptSnippet(text=\"library for vector storage of the chunks. Finally,\\xa0\\nlet's bring the entire rag pipeline together with\\xa0\\xa0\", start=172.16, duration=6.76), FetchedTranscriptSnippet(text=\"a nice user interface using Streamlit. We'll use\\xa0\\nthe LangChain connector to create and manipulate\\xa0\\xa0\", start=178.92, duration=5.72), FetchedTranscriptSnippet(text=\"prompt templates, retrieve relevant documents,\\xa0\\nand respond to the user. We'll also save messages\\xa0\\xa0\", start=184.64, duration=6.04), FetchedTranscriptSnippet(text='into the Streamlit context so we can print them\\xa0\\neach time that page is refreshed. And with that,\\xa0\\xa0', start=190.68, duration=5.8), FetchedTranscriptSnippet(text='we have our final chatbot in less than five\\xa0\\nminutes and around 100 lines of Python code.\\xa0\\xa0', start=196.48, duration=5.92), FetchedTranscriptSnippet(text=\"NVIDIA's open source connectors make it easy\\xa0\\nto try our new models with the latest NVIDIA\\xa0\\xa0\", start=202.4, duration=5.4), FetchedTranscriptSnippet(text='GPUs. You can also use NVIDIA AI Foundation\\xa0\\nEndpoints to access our GPUs directly from\\xa0\\xa0', start=207.8, duration=6.08), FetchedTranscriptSnippet(text=\"your applications. Best of all, they're free\\xa0\\nto use for up to 10,000 API transactions. Try\\xa0\\xa0\", start=213.88, duration=6.6), FetchedTranscriptSnippet(text=\"out this example and others by visiting NVIDIA's\\xa0\\ngenerative AI examples on GitHub and make sure\\xa0\\xa0\", start=220.48, duration=6.48), FetchedTranscriptSnippet(text='to check back in for our latest updates. We\\xa0\\nalso have the tools you need to expand upon\\xa0\\xa0', start=226.96, duration=5.2), FetchedTranscriptSnippet(text='these examples to build your own applications. To\\xa0\\ncontinue exploring how to use RAG, check out the\\xa0\\xa0', start=232.16, duration=6.08), FetchedTranscriptSnippet(text=\"links in this video's description. You'll find\\xa0\\ndocumentation, developer blogs, and much more!\", start=238.24, duration=9.48)], video_id='N_OOfkEWcOk', language='English', language_code='en', is_generated=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = [snippet.text for snippet in transcript.snippets]\n",
        "text_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIDl7VJMFem9",
        "outputId": "9304556b-1467-4eb8-8ec4-6ac85dfa8295"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Hi, my name is Rohan and I'm a senior solutions\\xa0\\narchitect working on Enterprise Generative AI at\\xa0\\xa0\",\n",
              " \"NVIDIA. Large language models, or LLMs, are taking\\xa0\\nthe world by storm. And now, they're easier to\\xa0\\xa0\",\n",
              " \"develop and deploy than ever, even without needing\\xa0\\nto host your own GPU infrastructure. Today, I'll\\xa0\\xa0\",\n",
              " 'be showing you how to design an enterprise-grade\\xa0\\nretrieval augmented generation, or RAG, pipeline\\xa0\\xa0',\n",
              " \"using NVIDIA's AI Foundation models. Since all of\\xa0\\nthe embedding and generation happens on NVIDIA AI\\xa0\\xa0\",\n",
              " \"Foundation endpoints, no local GPU is required.\\xa0\\nIf you'd like to experiment with open-source LLMs\\xa0\\xa0\",\n",
              " 'accelerated by the latest NVIDIA GPUs, check out\\xa0\\nthese endpoints. We host a collection of models\\xa0\\xa0',\n",
              " 'you can interact with through our UI. NVIDIA NeVA,\\xa0\\nthe NEMO visual and language assistant model,\\xa0\\xa0',\n",
              " 'is a great example. We can use NeVA for multimodal\\xa0\\nimage and language understanding through the UI,\\xa0\\xa0',\n",
              " \"but we can also access NVIDIA endpoints in our\\xa0\\napplications through API calls. Let's build a\\xa0\\xa0\",\n",
              " \"local RAG application that uses our AI foundation\\xa0\\nmodels. We'll need four main components. Firstly,\\xa0\\xa0\",\n",
              " 'a custom data loader for segmenting documents.\\xa0\\nSecondly, a text embedding model for converting\\xa0\\xa0',\n",
              " 'document chunks into vectors. Third, a\\xa0\\nvector database for storing these embeddings,\\xa0\\xa0',\n",
              " 'and finally, a large language model for\\xa0\\nresponse generation. In this example,\\xa0\\xa0',\n",
              " \"we'll be using a LangChain connector to NVIDIA's\\xa0\\nAI Foundation endpoints. So if you're already\\xa0\\xa0\",\n",
              " \"working with popular open-source frameworks like\\xa0\\nLangChain and LlamaIndex, you don't have to start\\xa0\\xa0\",\n",
              " \"over, since NVIDIA upstreams and maintains the\\xa0\\nconnectors. Let's get started. The first step is\\xa0\\xa0\",\n",
              " 'to make an API key for NGC. Click through to this\\xa0\\npage where you can sign up to use the APIs. After\\xa0\\xa0',\n",
              " 'you\\'ve logged in, click on \"Generate API Key\"\\xa0\\nto create a key you can use in your code. Now,\\xa0\\xa0',\n",
              " \"let's build the chat UI. We'll use Streamlit,\\xa0\\na popular open-source framework for building\\xa0\\xa0\",\n",
              " \"and sharing machine learning and data science\\xa0\\nweb apps. First, we'll focus on the custom data\\xa0\\xa0\",\n",
              " 'connector. Here, we have Streamlit code in Python\\xa0\\nto create a directory for uploading documents,\\xa0\\xa0',\n",
              " 'a browse and upload form, and a successful upload\\xa0\\nbanner. Next, we look at the text embedding model.\\xa0\\xa0',\n",
              " \"The model is hosted on NVIDIA's AI Foundation\\xa0\\nendpoints, so we can access it via the LangChain\\xa0\\xa0\",\n",
              " \"connector. Now, let's deploy a vector database\\xa0\\nto index our embeddings. In this code snippet,\\xa0\\xa0\",\n",
              " \"we'll create a vector store or load a saved\\xa0\\none from memory. To create a new vector store,\\xa0\\xa0\",\n",
              " 'split the documents into chunks based on a certain\\xa0\\nnumber of characters and then use the FAISS\\xa0\\xa0',\n",
              " \"library for vector storage of the chunks. Finally,\\xa0\\nlet's bring the entire rag pipeline together with\\xa0\\xa0\",\n",
              " \"a nice user interface using Streamlit. We'll use\\xa0\\nthe LangChain connector to create and manipulate\\xa0\\xa0\",\n",
              " \"prompt templates, retrieve relevant documents,\\xa0\\nand respond to the user. We'll also save messages\\xa0\\xa0\",\n",
              " 'into the Streamlit context so we can print them\\xa0\\neach time that page is refreshed. And with that,\\xa0\\xa0',\n",
              " 'we have our final chatbot in less than five\\xa0\\nminutes and around 100 lines of Python code.\\xa0\\xa0',\n",
              " \"NVIDIA's open source connectors make it easy\\xa0\\nto try our new models with the latest NVIDIA\\xa0\\xa0\",\n",
              " 'GPUs. You can also use NVIDIA AI Foundation\\xa0\\nEndpoints to access our GPUs directly from\\xa0\\xa0',\n",
              " \"your applications. Best of all, they're free\\xa0\\nto use for up to 10,000 API transactions. Try\\xa0\\xa0\",\n",
              " \"out this example and others by visiting NVIDIA's\\xa0\\ngenerative AI examples on GitHub and make sure\\xa0\\xa0\",\n",
              " 'to check back in for our latest updates. We\\xa0\\nalso have the tools you need to expand upon\\xa0\\xa0',\n",
              " 'these examples to build your own applications. To\\xa0\\ncontinue exploring how to use RAG, check out the\\xa0\\xa0',\n",
              " \"links in this video's description. You'll find\\xa0\\ndocumentation, developer blogs, and much more!\"]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_text = \" \".join(text_list)#.replace('\\xa0', ' ').replace('\\n', '')\n",
        "transcript_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "0x2AE2tTHqiW",
        "outputId": "4451bb05-f977-4850-88f6-fd99aaaa417e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi, my name is Rohan and I\\'m a senior solutions\\xa0\\narchitect working on Enterprise Generative AI at\\xa0\\xa0 NVIDIA. Large language models, or LLMs, are taking\\xa0\\nthe world by storm. And now, they\\'re easier to\\xa0\\xa0 develop and deploy than ever, even without needing\\xa0\\nto host your own GPU infrastructure. Today, I\\'ll\\xa0\\xa0 be showing you how to design an enterprise-grade\\xa0\\nretrieval augmented generation, or RAG, pipeline\\xa0\\xa0 using NVIDIA\\'s AI Foundation models. Since all of\\xa0\\nthe embedding and generation happens on NVIDIA AI\\xa0\\xa0 Foundation endpoints, no local GPU is required.\\xa0\\nIf you\\'d like to experiment with open-source LLMs\\xa0\\xa0 accelerated by the latest NVIDIA GPUs, check out\\xa0\\nthese endpoints. We host a collection of models\\xa0\\xa0 you can interact with through our UI. NVIDIA NeVA,\\xa0\\nthe NEMO visual and language assistant model,\\xa0\\xa0 is a great example. We can use NeVA for multimodal\\xa0\\nimage and language understanding through the UI,\\xa0\\xa0 but we can also access NVIDIA endpoints in our\\xa0\\napplications through API calls. Let\\'s build a\\xa0\\xa0 local RAG application that uses our AI foundation\\xa0\\nmodels. We\\'ll need four main components. Firstly,\\xa0\\xa0 a custom data loader for segmenting documents.\\xa0\\nSecondly, a text embedding model for converting\\xa0\\xa0 document chunks into vectors. Third, a\\xa0\\nvector database for storing these embeddings,\\xa0\\xa0 and finally, a large language model for\\xa0\\nresponse generation. In this example,\\xa0\\xa0 we\\'ll be using a LangChain connector to NVIDIA\\'s\\xa0\\nAI Foundation endpoints. So if you\\'re already\\xa0\\xa0 working with popular open-source frameworks like\\xa0\\nLangChain and LlamaIndex, you don\\'t have to start\\xa0\\xa0 over, since NVIDIA upstreams and maintains the\\xa0\\nconnectors. Let\\'s get started. The first step is\\xa0\\xa0 to make an API key for NGC. Click through to this\\xa0\\npage where you can sign up to use the APIs. After\\xa0\\xa0 you\\'ve logged in, click on \"Generate API Key\"\\xa0\\nto create a key you can use in your code. Now,\\xa0\\xa0 let\\'s build the chat UI. We\\'ll use Streamlit,\\xa0\\na popular open-source framework for building\\xa0\\xa0 and sharing machine learning and data science\\xa0\\nweb apps. First, we\\'ll focus on the custom data\\xa0\\xa0 connector. Here, we have Streamlit code in Python\\xa0\\nto create a directory for uploading documents,\\xa0\\xa0 a browse and upload form, and a successful upload\\xa0\\nbanner. Next, we look at the text embedding model.\\xa0\\xa0 The model is hosted on NVIDIA\\'s AI Foundation\\xa0\\nendpoints, so we can access it via the LangChain\\xa0\\xa0 connector. Now, let\\'s deploy a vector database\\xa0\\nto index our embeddings. In this code snippet,\\xa0\\xa0 we\\'ll create a vector store or load a saved\\xa0\\none from memory. To create a new vector store,\\xa0\\xa0 split the documents into chunks based on a certain\\xa0\\nnumber of characters and then use the FAISS\\xa0\\xa0 library for vector storage of the chunks. Finally,\\xa0\\nlet\\'s bring the entire rag pipeline together with\\xa0\\xa0 a nice user interface using Streamlit. We\\'ll use\\xa0\\nthe LangChain connector to create and manipulate\\xa0\\xa0 prompt templates, retrieve relevant documents,\\xa0\\nand respond to the user. We\\'ll also save messages\\xa0\\xa0 into the Streamlit context so we can print them\\xa0\\neach time that page is refreshed. And with that,\\xa0\\xa0 we have our final chatbot in less than five\\xa0\\nminutes and around 100 lines of Python code.\\xa0\\xa0 NVIDIA\\'s open source connectors make it easy\\xa0\\nto try our new models with the latest NVIDIA\\xa0\\xa0 GPUs. You can also use NVIDIA AI Foundation\\xa0\\nEndpoints to access our GPUs directly from\\xa0\\xa0 your applications. Best of all, they\\'re free\\xa0\\nto use for up to 10,000 API transactions. Try\\xa0\\xa0 out this example and others by visiting NVIDIA\\'s\\xa0\\ngenerative AI examples on GitHub and make sure\\xa0\\xa0 to check back in for our latest updates. We\\xa0\\nalso have the tools you need to expand upon\\xa0\\xa0 these examples to build your own applications. To\\xa0\\ncontinue exploring how to use RAG, check out the\\xa0\\xa0 links in this video\\'s description. You\\'ll find\\xa0\\ndocumentation, developer blogs, and much more!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def normalize_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Expand contractions\n",
        "    text = contractions.fix(text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "FQgCt9GGHvee"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_text = normalize_text(transcript_text)\n",
        "transcript_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "OADZOFfgKWgu",
        "outputId": "f1e696e0-7cb6-4a1f-e961-d45fceebf87b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi, my name is rohan and i am a senior solutions architect working on enterprise generative ai at nvidia. large language models, or llms, are taking the world by storm. and now, they are easier to develop and deploy than ever, even without needing to host your own gpu infrastructure. today, i will be showing you how to design an enterprise-grade retrieval augmented generation, or rag, pipeline using nvidia\\'s ai foundation models. since all of the embedding and generation happens on nvidia ai foundation endpoints, no local gpu is required. if you would like to experiment with open-source llms accelerated by the latest nvidia gpus, check out these endpoints. we host a collection of models you can interact with through our ui. nvidia neva, the nemo visual and language assistant model, is a great example. we can use neva for multimodal image and language understanding through the ui, but we can also access nvidia endpoints in our applications through api calls. let us build a local rag application that uses our ai foundation models. we will need four main components. firstly, a custom data loader for segmenting documents. secondly, a text embedding model for converting document chunks into vectors. third, a vector database for storing these embeddings, and finally, a large language model for response generation. in this example, we will be using a langchain connector to nvidia\\'s ai foundation endpoints. so if you are already working with popular open-source frameworks like langchain and llamaindex, you do not have to start over, since nvidia upstreams and maintains the connectors. let us get started. the first step is to make an api key for ngc. click through to this page where you can sign up to use the apis. after you have logged in, click on \"generate api key\" to create a key you can use in your code. now, let us build the chat ui. we will use streamlit, a popular open-source framework for building and sharing machine learning and data science web apps. first, we will focus on the custom data connector. here, we have streamlit code in python to create a directory for uploading documents, a browse and upload form, and a successful upload banner. next, we look at the text embedding model. the model is hosted on nvidia\\'s ai foundation endpoints, so we can access it via the langchain connector. now, let us deploy a vector database to index our embeddings. in this code snippet, we will create a vector store or load a saved one from memory. to create a new vector store, split the documents into chunks based on a certain number of characters and then use the faiss library for vector storage of the chunks. finally, let us bring the entire rag pipeline together with a nice user interface using streamlit. we will use the langchain connector to create and manipulate prompt templates, retrieve relevant documents, and respond to the user. we will also save messages into the streamlit context so we can print them each time that page is refreshed. and with that, we have our final chatbot in less than five minutes and around 100 lines of python code. nvidia\\'s open source connectors make it easy to try our new models with the latest nvidia gpus. you can also use nvidia ai foundation endpoints to access our gpus directly from your applications. best of all, they are free to use for up to 10,000 api transactions. try out this example and others by visiting nvidia\\'s generative ai examples on github and make sure to check back in for our latest updates. we also have the tools you need to expand upon these examples to build your own applications. to continue exploring how to use rag, check out the links in this video\\'s description. you will find documentation, developer blogs, and much more!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creaing a function to extract and clean a YouTube transcript from a video URL"
      ],
      "metadata": {
        "id": "lmuoWiNKK6l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_transcript(url):\n",
        "  match = re.search(r\"(?:v=|\\/)([0-9A-Za-z_-]{11})\", url)\n",
        "  video_id = match.group(1) if match else None\n",
        "  yt = YouTubeTranscriptApi()\n",
        "  transcript = yt.fetch(video_id)\n",
        "  text_list = [snippet.text for snippet in transcript.snippets]\n",
        "  return normalize_text( \" \".join(text_list))"
      ],
      "metadata": {
        "id": "pncTEJF8KaF-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_text = get_video_transcript(\"https://youtu.be/N_OOfkEWcOk?si=QMAEhom_hQP7qe_a\")\n",
        "transcript_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "014KABHrK3v2",
        "outputId": "b2e9b993-37d9-4f25-d897-6cafef595d20"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi, my name is rohan and i am a senior solutions architect working on enterprise generative ai at nvidia. large language models, or llms, are taking the world by storm. and now, they are easier to develop and deploy than ever, even without needing to host your own gpu infrastructure. today, i will be showing you how to design an enterprise-grade retrieval augmented generation, or rag, pipeline using nvidia\\'s ai foundation models. since all of the embedding and generation happens on nvidia ai foundation endpoints, no local gpu is required. if you would like to experiment with open-source llms accelerated by the latest nvidia gpus, check out these endpoints. we host a collection of models you can interact with through our ui. nvidia neva, the nemo visual and language assistant model, is a great example. we can use neva for multimodal image and language understanding through the ui, but we can also access nvidia endpoints in our applications through api calls. let us build a local rag application that uses our ai foundation models. we will need four main components. firstly, a custom data loader for segmenting documents. secondly, a text embedding model for converting document chunks into vectors. third, a vector database for storing these embeddings, and finally, a large language model for response generation. in this example, we will be using a langchain connector to nvidia\\'s ai foundation endpoints. so if you are already working with popular open-source frameworks like langchain and llamaindex, you do not have to start over, since nvidia upstreams and maintains the connectors. let us get started. the first step is to make an api key for ngc. click through to this page where you can sign up to use the apis. after you have logged in, click on \"generate api key\" to create a key you can use in your code. now, let us build the chat ui. we will use streamlit, a popular open-source framework for building and sharing machine learning and data science web apps. first, we will focus on the custom data connector. here, we have streamlit code in python to create a directory for uploading documents, a browse and upload form, and a successful upload banner. next, we look at the text embedding model. the model is hosted on nvidia\\'s ai foundation endpoints, so we can access it via the langchain connector. now, let us deploy a vector database to index our embeddings. in this code snippet, we will create a vector store or load a saved one from memory. to create a new vector store, split the documents into chunks based on a certain number of characters and then use the faiss library for vector storage of the chunks. finally, let us bring the entire rag pipeline together with a nice user interface using streamlit. we will use the langchain connector to create and manipulate prompt templates, retrieve relevant documents, and respond to the user. we will also save messages into the streamlit context so we can print them each time that page is refreshed. and with that, we have our final chatbot in less than five minutes and around 100 lines of python code. nvidia\\'s open source connectors make it easy to try our new models with the latest nvidia gpus. you can also use nvidia ai foundation endpoints to access our gpus directly from your applications. best of all, they are free to use for up to 10,000 api transactions. try out this example and others by visiting nvidia\\'s generative ai examples on github and make sure to check back in for our latest updates. we also have the tools you need to expand upon these examples to build your own applications. to continue exploring how to use rag, check out the links in this video\\'s description. you will find documentation, developer blogs, and much more!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model using GROQ API"
      ],
      "metadata": {
        "id": "zmRIuybisx3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Safe way to set the API key\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h8unPRqoQWt",
        "outputId": "63664ce6-42e9-4220-df40-5dec7d7bfd9f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=1,\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"]\n",
        ")"
      ],
      "metadata": {
        "id": "3u9jKihenMjD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert summarizer that generates clear, accurate, and concise summaries of YouTube videos based on their transcripts. \\\n",
        "    Do not include titles, bullet points, or formatting only return plain text.\"),\n",
        "    (\"human\", \"Please summarize the following YouTube video transcript. Transcript: {transcript}\")\n",
        "])\n",
        "\n",
        "RunnableVideoTranscript = RunnableLambda(get_video_transcript)\n",
        "parser = StrOutputParser()\n",
        "chain = RunnableVideoTranscript | template | llm | parser\n",
        "response = chain.invoke(\"https://youtu.be/N_OOfkEWcOk?si=QMAEhom_hQP7qe_a\")"
      ],
      "metadata": {
        "id": "rRZRUja5Tk89"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Olw29LlNpQ3V",
        "outputId": "93b3804f-6da3-400c-8b58-50644dc9f824"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Rohan, a senior solutions architect at NVIDIA, is discussing the design of an enterprise-grade retrieval augmented generation, or RAG, pipeline using NVIDIA's AI Foundation Models. He explains that users can experiment with open-source LLMs accelerated by the latest NVIDIA GPUs via NVIDIA's AI Foundation Endpoints, which don't require local GPU infrastructure. \\n\\nTo build a local RAG application, four main components are needed: a custom data loader, a text embedding model, a vector database, and a large language model for response generation. Rohan demonstrates how to use NVIDIA's AI Foundation Endpoints through the LangChain connector to perform these tasks. \\n\\nThe steps involve creating an API key through the NVIDIA NGC, building a chat UI with Streamlit, and integrating the custom data connector, text embedding model, and vector database. The final RAG pipeline is then brought together into a user-friendly interface that retrieves relevant documents and responds to user prompts. \\n\\nRohan emphasizes the ease and flexibility of using NVIDIA's open-source connectors and AI Foundation Endpoints, allowing users to try new models with the latest NVIDIA GPUs for free up to 10,000 API transactions. He encourages viewers to experiment with NVIDIA's Generative AI Examples on GitHub and to explore further resources for building their own applications.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model using openAI API"
      ],
      "metadata": {
        "id": "OijRwTkOs5qN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
      ],
      "metadata": {
        "id": "CdN24viwtNYS"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    base_url=\"https://api.groq.com/openai/v1\",  # Groq endpoint\n",
        "    model=\"openai/gpt-oss-20b\",\n",
        "    temperature=0.3\n",
        ")"
      ],
      "metadata": {
        "id": "xoKvlxMfnnEQ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert summarizer that generates clear, accurate, and concise summaries of YouTube videos based on their transcripts. \\\n",
        "    Do not include titles, bullet points, or formatting only return plain text.\"),\n",
        "    (\"human\", \"Please summarize the following YouTube video transcript. Transcript: {transcript}\")\n",
        "])\n",
        "\n",
        "RunnableVideoTranscript = RunnableLambda(get_video_transcript)\n",
        "parser = StrOutputParser()\n",
        "chain = RunnableVideoTranscript | template | llm | parser\n",
        "response = chain.invoke(\"https://youtu.be/N_OOfkEWcOk?si=QMAEhom_hQP7qe_a\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "qON-CidDuMEF",
        "outputId": "19bfce59-e723-4f25-b1f0-ec19631973e4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rohan, a senior solutions architect at NVIDIA, explains how to build an enterprise‑grade retrieval‑augmented generation (RAG) pipeline using NVIDIA’s AI Foundation models. He outlines the four core components: a custom data loader to segment documents, an embedding model that converts text chunks into vectors, a vector database (using FAISS) to store those embeddings, and a large language model to generate responses. Rohan shows how to use LangChain connectors to access NVIDIA’s hosted endpoints, and how to assemble the pipeline in under five minutes with about 100 lines of Python, including a Streamlit user interface for uploading documents, embedding them, storing the vectors, and querying the LLM. He notes that the entire process requires no local GPU, that NVIDIA offers free access for up to 10,000 API transactions, and that the open‑source connectors and example code on GitHub enable developers to quickly prototype and expand their own RAG applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Web Interface for our Youtube Summarizer"
      ],
      "metadata": {
        "id": "IX3duWqMzrtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_web_interface(chain):\n",
        "    def generate_response(user_input):\n",
        "        return chain.invoke(user_input)\n",
        "\n",
        "    with gr.Blocks() as app:\n",
        "        with gr.Row():\n",
        "            text_input = gr.Textbox(label=\"User Message\", placeholder=\"Ready to summarize? Paste a YouTube link.\")\n",
        "\n",
        "        output_area = gr.Textbox(label=\"AI Response\", interactive=False, lines=10)\n",
        "\n",
        "        text_input.submit(generate_response, inputs=text_input, outputs=output_area)\n",
        "\n",
        "    return app"
      ],
      "metadata": {
        "id": "NkGjsUnCz3lY"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = create_web_interface(chain)\n",
        "app.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "6thesQ6U18F1",
        "outputId": "a663a0ac-c302-4d9e-eb82-9599426098ce"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://64eaace08382c1b30e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://64eaace08382c1b30e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8Acg1ml2Bwn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}